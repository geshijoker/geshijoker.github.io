---
title: 'Creating customized colorbar'
date: 2012-03-06
permalink: /posts/2022/03/blog-post-9/
tags:
  - Visualization
  - Matplotlib
  - Colormap
---

When creating heatmaps, we usually have

Epoch vs Batch Size vs Iterations
======
<a href="https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9">Reference</a>
One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE. The batch size is the total number of training examples present in a single batch. Batches are parts of dataset for the big dataset is not able to fits in the memory. Iteration is the total number of training examples present in a single batch.

What is the curse of dimensionality?
======
```
import matplotlib as mpl
import matplotlib.pyplot as plt 
from matplotlib import cm
import matplotlib.colors as mcol
# matplotlib.use('Agg')

plt.imshow(mixup_example, cmap=plt.get_cmap('cool'))
#plt.imshow(mixup_example, cmap=cm1)
plt.axis('off')
plt.savefig('mixup.png', bbox_inches='tight',pad_inches = 0)
```
<a href="http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/">Reference</a>
Normally, the more features we use, the higher the likelihood that we can successfully separate the classes perfectly. Using too many features results in overfitting. The classifier starts learning exceptions that are specific to the training data and do not generalize well when new data is encountered. As the dimensionality increases, a larger percentage of the training data resides in the corners of the feature space. Then we should considering reduce the dimension of the features using such as Principal Component Analysis (PCA).

Explain over- and under-fitting and how to combat them?
======
```
cm1 = mcol.LinearSegmentedColormap.from_list("MyCmapName",[[1,0.4,0.4],[0.4,0.4,1]])
cnorm = mcol.Normalize(vmin=0,vmax=1)
cpick = cm.ScalarMappable(norm=cnorm,cmap=cm1)
cpick.set_array([])
```
Overfitting is when a model fitts exactly close to a specific set of training data while it fails to fit into other sets of data. Underfitting occurs when a statistical model cannot adequately capture the underlying structure of the data. Underfitting would occur, for example, when fitting a linear model to non-linear data.

What is regularization, why do we use it, and give some examples of common methods?
======
<a href="https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a">Reference</a>
This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting. 


Explain Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE)?
======
Both methods are aiming to do dimensionality reduction and are widely used in visualization. 
PCA selects the most significant orthogonal components to decompose data, which means the data can be represented as a weighted average of principal components. The process is: normalize data, computecovariance Matrix, get eigenvectors and eigenvalues with SVD, arrange eigenvalues from large to small, project data to eigenvectors that are selected by eigenvalues. 
t-SNE, Contrary to PCA, is not a mathematical technique but a probablistic one. It tries to solve how to best represent input data using less dimensions by matching distributions.

What is data normalization and why do we need it?
======
The data normalization makes all features weighted equally. It's used to rescale values to fit in a specific range to assure better convergence during backpropagation. In general, it boils down to subtracting the mean of each data point and dividing by its standard deviation.

Explain the benefits of dimensionality reduction?
======
(1) Reduce the storage space needed (2) Speed up computation, less dimensions mean less computing (3) Remove redundant features (4) Good for visualization (5) Too many features or too complex a model can lead to overfitting.

How do you handle missing or corrupted data in a dataset?
======
You could find missing/corrupted data in a dataset and either drop those rows or columns, or decide to replace them with another value.

Explain decision tree and random forest.
======
<a href="https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d">Reference</a>
A decision tree is a flowchart like tree model leading to a prediction. In this procedure all the features are considered and different split points are tried and tested using a cost function. The split with the best cost (or lowest cost) is selected, and do recursive binary splitting. The random forest is an ensemble of Decision Trees. Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction. Random Forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model.

What's the popular clustering algorithms
======
<a href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68">Reference</a>
K-Means Clustering, Mean-Shift Clustering, Spatial Clustering, EM Clustering with GMM, Hierarchical Agglomerative Clustering.

Good luck!
------